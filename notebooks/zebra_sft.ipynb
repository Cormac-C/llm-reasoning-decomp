{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import pytorch_utils as torch_utils\n",
    "from peft import LoraConfig\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.train\n",
    "import src.model\n",
    "\n",
    "importlib.reload(src.train)\n",
    "importlib.reload(src.model)\n",
    "\n",
    "from src.train import sft_train_lora\n",
    "from src.model import identify_target_modules\n",
    "from data.zebra import Zebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847e2b6aaffd42ca86490722cf54ce37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "# insert your huggingface token here\n",
    "dataset = Zebra(tokenizer=tokenizer, hf_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2, 18377,   345,    32,   195,  3960,     6, 30390,   112,     7,\n",
       "           195,    31,   314,     7,   235,     6,    25,   450,    31,   420,\n",
       "             5,  2014,     4,  4028,   790,    16,  9533,    30,    10,   430,\n",
       "           621,     4,  4028,   790,    34,    10,  2216, 21643,    13,   349,\n",
       "             9,     5,   511, 12720,    35, 50118,   111,  4028,   621,    34,\n",
       "            10,  2216,   766,    35, 22209, 22611, 48948, 22209, 43714, 48948,\n",
       "         22209, 25158, 48948, 22209, 24375, 48948, 22209,  8138,   282,   279,\n",
       "         12905, 50118,   111,    20,    82,    32,     9,   632,  2192,    35,\n",
       "         22209, 21651, 45618, 48948, 22209,  2403,   397, 48948, 22209,   417,\n",
       "          1728, 48948, 22209,   428,  3961, 48948, 22209,  4184, 12820, 12905,\n",
       "         50118,   111,  1806,    33,  2216,  2674,  1040, 23409,    35, 22209,\n",
       "           506, 40650, 48948, 22209,  5605, 10486, 48948, 22209,  5638,  2389,\n",
       "         48948, 22209,  4783, 34821, 48948, 22209, 33749, 11845, 12905, 50118,\n",
       "           111,  7632,    34,   402,  2216,    13,  4592,    35, 22209,   620,\n",
       "           853, 25950, 48948, 22209,  6504, 12146,  7134, 48948, 22209,   642,\n",
       "         35280, 48948, 22209,  4182, 44129, 48948, 22209,   620,  2753, 12905,\n",
       "         50118,   111,  4028,   621,    34,    10,  2674,  3195,    35, 22209,\n",
       "          2050, 48948, 22209, 12354, 48948, 22209, 22128, 48948, 22209, 34103,\n",
       "         48948, 22209,  9830, 12905, 50118,   111,    20,    82,   489,  2216,\n",
       "          3122,    35, 22209, 15886, 48948, 22209, 16319, 48948, 22209,  8729,\n",
       "         48948, 22209, 19471, 48948, 22209,  9106, 12905, 50118, 50118, 48342,\n",
       "          2893,  3663,    35, 50118,   134,     4,    20,   621,    54,  6138,\n",
       "          8235,  2799,    16,     5, 11814,     4, 50118,   176,     4,    20,\n",
       "          4758, 16095,     8,     5,   621,    54,  6138, 24613,  2799,    32,\n",
       "           220,     7,   349,    97,     4, 50118,   246,     4,    20,  1859,\n",
       "            16,  3045,     4, 50118,   306,     4,    20,   621,    54,  6138,\n",
       "          5718,    16,  3045,     4, 50118,   245,     4,    20,   621,  1060,\n",
       "          2674,  3195,    16,  2272,    16,  2155,     4, 50118,   401,     4,\n",
       "           345,    16,    65,   790,   227,     5, 21767,     8,     5,   621,\n",
       "            54,    16,    10,  9366, 16095,     4, 50118,   406,     4,    20,\n",
       "           621,    54,  6138,  2440,    16,  6152,     7,     5,   314,     9,\n",
       "             5, 21767,     4, 50118,   398,     4,    20,   621,    54,  6138,\n",
       "          4441, 20346,  7134,    16,  6152,     7,     5,   314,     9,     5,\n",
       "         11814,     4, 50118,   466,     4,    20,   621,    54,  6138,     5,\n",
       "         33362, 19969,    16,  2155,     4, 50118,   698,     4,    20,   621,\n",
       "            54,  4719,  8087,    16, 12085,     4, 50118,  1225,     4,    20,\n",
       "          3539, 27330,    16,  2024,   314,     9,     5,   621,    54,  6138,\n",
       "          2866, 11845,  2799,     4, 50118,  1092,     4,   345,    16,    65,\n",
       "           790,   227,     5, 11814,     8, 11816,     4, 50118,  1558,     4,\n",
       "            20,   621,    54,  6138,  9884,  2799,    16,     5,  1089,   621,\n",
       "             4, 50118,  1570,     4,   345,    32,    80,  3960,   227,     5,\n",
       "         11814,     8, 12085,     4, 50118,   996,     4,    20,  5103, 13456,\n",
       "            16,     5,   621,  1060,  2674,  3195,    16,  1275,     4, 50118,\n",
       "          1549,     4,    20,  2335,  1945,    16,  2024,   314,     9,     5,\n",
       "          3539, 27330,     4, 50118,  1360,     4,    20,   621,    54,  6138,\n",
       "             5, 24571,    16,     5, 11814,     4, 50118,     4,  3401,  6136,\n",
       "            13,     5,   507,  9804,     4, 22560, 31652,    35,    20,  2472,\n",
       "            16,    25,  3905,    35, 50118,  1121,   790,   112,     6,   766,\n",
       "            16,  3045,     6, 26241,    16,   821,  7043,     6,  1040, 44205,\n",
       "            16,  9001,     6,   689,    16, 20346,  7134,     6,  3195,    16,\n",
       "          5718,     6,  3477,    16,  2335,     4, 50118,  1121,   790,   132,\n",
       "             6,   766,    16,  2954,     6, 26241,    16,  3486, 45618,     6,\n",
       "          1040, 44205,    16,  8235,     6,   689,    16, 24571,     6,  3195,\n",
       "            16,  2440,     6,  3477,    16,  3539,     4, 50118,  1121,   790,\n",
       "           155,     6,   766,    16,  2155,     6, 26241,    16,   385,  1728,\n",
       "             6,  1040, 44205,    16,  2866, 11845,     6,   689,    16, 33362,\n",
       "             6,  3195,    16,  2272,     6,  3477,    16,  4758,     4, 50118,\n",
       "          1121,   790,   204,     6,   766,    16, 11816,     6, 26241,    16,\n",
       "          3514, 12820,     6,  1040, 44205,    16, 24613,     6,   689,    16,\n",
       "         14351, 25950,     6,  3195,    16,  1275,     6,  3477,    16,  5103,\n",
       "             4, 50118,  1121,   790,   195,     6,   766,    16, 12085,     6,\n",
       "         26241,    16,   741,  3961,     6,  1040, 44205,    16,  9884,     6,\n",
       "           689,    16,  9366,     6,  3195,    16,  1104,     6,  3477,    16,\n",
       "          5253,     4, 50118]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = identify_target_modules(model, name_segment='self_attn')\n",
    "print(target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    target_modules=target_modules,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/reasoning-decomp/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa79c8bebf24adc92ea42ffead69bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_train_lora(\n",
    "    base_model=model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"facebook/opt-350m\"),\n",
    "    adapter_name=\"sft_lora\",\n",
    "    response_template=\" ### Answer:\",\n",
    "    lora_config=lora_config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning-decomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
